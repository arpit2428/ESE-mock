{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[],"authorship_tag":"ABX9TyP/MIeGpGiF3/VCDoK88s4j"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1 code\n# ================================\n# FINAL EXAM-SAFE ML CLASSIFICATION CODE\n# (Binary + Multiclass supported)\n# ================================\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ================================\n# 1. LOAD DATA\n# ================================\ntrain = pd.read_csv(\"/kaggle/input/mse-2-ai-201-b-aiml-a/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/mse-2-ai-201-b-aiml-a/test.csv\")\n\n# ❌ ERROR FIX:\n# If path error comes → check dataset name in Kaggle input\n\n# ================================\n# 2. BASIC EDA (OPTIONAL IN EXAM)\n# ================================\nprint(train.head())\nprint(train.info())\nprint(train.isnull().sum())\n\n# ❌ ERROR FIX:\n# If 'Class' column not found → print(train.columns) and change name\nprint(train['Class'].value_counts())\n\n# ================================\n# 3. FEATURES & TARGET\n# ================================\ny = train[\"Class\"]          # ❌ If KeyError → change target column name\nX = train.drop(\"Class\", axis=1)\n\nnum_cols = X.select_dtypes(include=['int64','float64']).columns\ncat_cols = X.select_dtypes(include=['object']).columns\n\n# ================================\n# 4. CATEGORY-WISE OUTLIER CAPPING\n# ================================\ndef cap_outliers_categorywise_all(df, cat_col, num_cols):\n    df = df.copy()\n    for col in num_cols:\n        Q1 = df.groupby(cat_col)[col].transform(lambda x: x.quantile(0.25))\n        Q3 = df.groupby(cat_col)[col].transform(lambda x: x.quantile(0.75))\n        IQR = Q3 - Q1\n        lower = Q1 - 1.5 * IQR\n        upper = Q3 + 1.5 * IQR\n        df[col] = df[col].clip(lower, upper)\n    return df\n\n# ❌ ERROR FIX:\n# If groupby error / time issue → COMMENT THIS WHOLE LOOP\nfor c in cat_cols:\n    X = cap_outliers_categorywise_all(X, c, num_cols)\n    test = cap_outliers_categorywise_all(test, c, num_cols)\n\n# ================================\n# 5. HANDLE MISSING VALUES\n# ================================\n\n# Numeric columns (SAFE – NEVER FAILS)\nX[num_cols] = X[num_cols].fillna(X[num_cols].median())\ntest[num_cols] = test[num_cols].fillna(test[num_cols].median())\n\n# Categorical columns (EXAM-SAFE VERSION)\n# ❌ If ANY error comes → COMMENT THIS BLOCK COMPLETELY\nif len(cat_cols) > 0:\n    for col in cat_cols:\n        if X[col].isnull().any():\n            X[col].fillna(X[col].mode()[0], inplace=True)\n        if test[col].isnull().any():\n            test[col].fillna(test[col].mode()[0], inplace=True)\n\n# ================================\n# 6. RESET INDEX\n# ================================\nX = X.reset_index(drop=True)\ntest = test.reset_index(drop=True)\n\n# ================================\n# 7. LABEL ENCODE TARGET\n# ================================\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\n# ✔ Multiclass handled automatically\n\n# ================================\n# 8. PREPROCESS + MODEL\n# ================================\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n        (\"num\", \"passthrough\", num_cols)\n    ]\n)\n\nmodel = Pipeline(steps=[\n    (\"preprocess\", preprocess),\n    (\"clf\", RandomForestClassifier(random_state=42))\n])\n\n# ================================\n# 9. TRAIN / VALID SPLIT\n# ================================\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y_encoded, test_size=0.2, random_state=42\n)\n\n# ================================\n# 10. TRAIN MODEL\n# ================================\nmodel.fit(X_train, y_train)\n\n# ================================\n# 11. EVALUATION\n# ================================\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\ny_pred = model.predict(X_val)\ny_prob = model.predict_proba(X_val)\n\nprint(\"Accuracy :\", accuracy_score(y_val, y_pred))\nprint(\"Precision:\", precision_score(y_val, y_pred, average='macro'))\nprint(\"Recall   :\", recall_score(y_val, y_pred, average='macro'))\nprint(\"F1 Score :\", f1_score(y_val, y_pred, average='macro'))\n\n# ❌ If ROC AUC error comes → COMMENT THIS LINE\nprint(\"ROC AUC  :\", roc_auc_score(y_val, y_prob, multi_class='ovr'))\n\n# ================================\n# 12. TRAIN FULL MODEL\n# ================================\nmodel.fit(X, y_encoded)\n\n# ================================\n# 13. TEST PREDICTION\n# ================================\ntest_pred = model.predict(test)\ntest_pred_labels = le.inverse_transform(test_pred)\n\n# ================================\n# 14. SUBMISSION FILE\n# ================================\n\n# ❌ If 'id' column missing → replace test[\"id\"] with range(len(test))\nsubmission = pd.DataFrame({\n    \"id\": test[\"id\"],\n    \"NObeyesdad\": test_pred_labels\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"submission.csv CREATED SUCCESSFULLY!\")\n","metadata":{"id":"Q4JJdvCPuf8p","executionInfo":{"status":"ok","timestamp":1766082732016,"user_tz":-330,"elapsed":37,"user":{"displayName":"ARNAV GUPTA","userId":"11094490203888777867"}}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# =========================================================\n# UNIVERSAL TABULAR CLASSIFICATION TEMPLATE WITH VISUALS\n# Change only TRAIN_PATH, TEST_PATH, TARGET_COL, ID_COL\n# =========================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n\n\nTRAIN_PATH = \"/kaggle/input/mock-test-2-mse-2/train.csv\"\nTEST_PATH  = \"/kaggle/input/mock-test-2-mse-2/test.csv\"\nTARGET_COL = \"Status\"\nID_COL     = \"id\"\n\n\n# ---------------- CHANGE ONLY THESE ----------------\n# TRAIN_PATH = \"/kaggle/input/mock-test-2-mse-2/test.csv\"\n# TEST_PATH  = \"/kaggle/input/mock-test-2-mse-2/test.csv\"\n# TARGET_COL = \"Status\"\n# ID_COL     = \"id\"      # set None if not present\n# --------------------------------------------------\n\n# ================= LOAD DATA ======================\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)\n\ntrain.columns = train.columns.str.strip()\ntest.columns = test.columns.str.strip()\n\n# ================= BASIC VISUALIZATION =============\nprint(\"\\nDataset Shape:\", train.shape)\nprint(\"\\nTarget Distribution:\")\nprint(train[TARGET_COL].value_counts())\n\nplt.figure(figsize=(6,4))\ntrain[TARGET_COL].value_counts().plot(kind=\"bar\")\nplt.title(\"Target Distribution\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# ================= HANDLE ID ======================\ntest_ids = None\nif ID_COL and ID_COL in test.columns:\n    test_ids = test[ID_COL]\n    test = test.drop(columns=[ID_COL])\n\nif ID_COL and ID_COL in train.columns:\n    train = train.drop(columns=[ID_COL])\n\n# ================= SPLIT X & y ===================\nX = train.drop(columns=[TARGET_COL])\ny = train[TARGET_COL]\n\n# ================= COLUMN TYPES ==================\nnum_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\ncat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n\n# ================= MISSING VALUE VISUAL ===========\nplt.figure(figsize=(8,4))\ntrain.isnull().sum().sort_values(ascending=False).head(10).plot(kind=\"bar\")\nplt.title(\"Top Missing Values per Column\")\nplt.ylabel(\"Missing Count\")\nplt.show()\n\n# ================= PREPROCESSOR ==================\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", Pipeline([\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", StandardScaler())\n        ]), num_cols),\n\n        (\"cat\", Pipeline([\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n        ]), cat_cols)\n    ]\n)\n\n# ================= MODELS ========================\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n    \"Random Forest\": RandomForestClassifier(n_estimators=300, random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n    \"SVM\": SVC(probability=True)\n}\n\nbest_model = None\nbest_score = -1\nbest_name = \"\"\n\n# ================= TRAIN & SELECT =================\nfor name, clf in models.items():\n    pipeline = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"classifier\", clf)\n    ])\n\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42,\n        stratify=y if y.nunique() < 20 else None\n    )\n\n    pipeline.fit(X_train, y_train)\n    preds = pipeline.predict(X_val)\n    score = accuracy_score(y_val, preds)\n\n    print(f\"{name} Accuracy: {score:.4f}\")\n\n    if score > best_score:\n        best_score = score\n        best_model = pipeline\n        best_name = name\n        best_preds = preds\n        best_yval = y_val\n\nprint(\"\\nBEST MODEL:\", best_name)\n\n# ================= CONFUSION MATRIX VISUAL =========\ncm = confusion_matrix(best_yval, best_preds)\n\nplt.figure(figsize=(5,4))\nplt.imshow(cm, cmap=\"Blues\")\nplt.title(f\"Confusion Matrix ({best_name})\")\nplt.colorbar()\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\nplt.show()\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(best_yval, best_preds))\n\n# ================= FINAL TRAIN ====================\nbest_model.fit(X, y)\n\n\n\n# ================= TEST PREDICTION (PROBABILITIES) ======\nsample_sub = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/sample_submission.csv\")\n\nprobs = best_model.predict_proba(test)\n\nsubmission = pd.DataFrame(\n    probs,\n    columns=sample_sub.columns.drop(\"id\")   # Status_C, Status_CL, Status_D\n)\n\nsubmission.insert(0, \"id\", test_ids.values)\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"submission.csv CREATED CORRECTLY (PROBABILITIES)!\")","metadata":{"id":"aYkskkgbu8Kd","executionInfo":{"status":"ok","timestamp":1766083140029,"user_tz":-330,"elapsed":12,"user":{"displayName":"ARNAV GUPTA","userId":"11094490203888777867"}}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ============================================\n# MULTI-CLASS LOG LOSS – FULL KAGGLE CODE\n# ============================================\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss\nfrom lightgbm import LGBMClassifier\n\n# ============================================\n# 1. LOAD DATA\n# ============================================\n\ntrain = pd.read_csv(\"/kaggle/input/your-dataset-name/train.csv\")\ntest  = pd.read_csv(\"/kaggle/input/your-dataset-name/test.csv\")\n\nprint(train.shape, test.shape)\n\n# ============================================\n# 2. TARGET & FEATURES\n# ============================================\n\nTARGET = \"Status\"\nID_COL = \"id\"\n\nX = train.drop([TARGET], axis=1)\ny = train[TARGET]\n\n# ============================================\n# 3. LABEL ENCODING (C, CL, D → 0,1,2)\n# ============================================\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\nprint(\"Class mapping:\")\nfor cls, val in zip(le.classes_, range(len(le.classes_))):\n    print(cls, \"→\", val)\n\n# ============================================\n# 4. HANDLE CATEGORICAL FEATURES\n# ============================================\n\ncat_cols = X.select_dtypes(include=\"object\").columns\n\nfor col in cat_cols:\n    combined = pd.concat([X[col], test[col]], axis=0)\n    le_col = LabelEncoder()\n    le_col.fit(combined.astype(str))\n    X[col] = le_col.transform(X[col].astype(str))\n    test[col] = le_col.transform(test[col].astype(str))\n\n# ============================================\n# 5. TRAIN–VALIDATION SPLIT\n# ============================================\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n)\n\n# ============================================\n# 6. MODEL (MULTI-CLASS PROBABILITY)\n# ============================================\n\nmodel = LGBMClassifier(\n    objective=\"multiclass\",\n    num_class=3,\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    random_state=42\n)\n\nmodel.fit(X_train, y_train)\n\n# ============================================\n# 7. VALIDATION LOG LOSS\n# ============================================\n\nval_preds = model.predict_proba(X_val)\nval_loss = log_loss(y_val, val_preds)\n\nprint(\"Validation Log Loss:\", val_loss)\n\n# ============================================\n# 8. TRAIN ON FULL DATA\n# ============================================\n\nmodel.fit(X, y_encoded)\n\n# ============================================\n# 9. TEST PREDICTIONS (PROBABILITIES)\n# ============================================\n\ntest_preds = model.predict_proba(test)\n\n# ============================================\n# 10. CREATE SUBMISSION FILE\n# ============================================\n\nsubmission = pd.DataFrame({\n    \"id\": test[ID_COL],\n    \"Status_C\":  test_preds[:, le.transform([\"C\"])[0]],\n    \"Status_CL\": test_preds[:, le.transform([\"CL\"])[0]],\n    \"Status_D\":  test_preds[:, le.transform([\"D\"])[0]],\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"submission.csv created successfully!\")\nprint(submission.head())\n","metadata":{"id":"3vJhXl3Iwcp_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"k7ahnM2yw0mS"},"outputs":[],"execution_count":null}]}